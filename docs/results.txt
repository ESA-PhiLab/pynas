The proposed hardware-aware NAS framework was evaluated using a burned area segmentation task based on a custom dataset derived from Sentinel-2 imagery and simulated to replicate the onboard conditions of the Φ-sat-2 satellite. The dataset was designed with global geographic coverage and class-balanced annotations across four categories: Background, Burned Area, Cloud, and Water Body. To emulate realistic onboard data acquisition, we applied a comprehensive preprocessing pipeline that included spectral degradation, spatial resampling to 4.75 m, noise injection, and sensor-specific distortions. This ensured that training and evaluation reflected the constraints and conditions of spaceborne systems. Model performance was assessed using the mean Intersection over Union (mIoU), a standard metric for semantic segmentation. Experiments were carried out on two distinct hardware platforms—NVIDIA Jetson AGX Orin™ and Intel® Movidius™ Myriad™ X—to demonstrate the adaptability of the NAS framework to different edge environments. The Jetson Orin represents a high-throughput, mixed-precision platform, whereas the Myriad X is optimized for ultra-low-power 16-bit fixed-point inference. On the Jetson AGX Orin™, our NAS framework discovered an architecture that achieved an mIoU of 0.845 while running at 168.1 frames per second (FPS), using only 34.2K parameters. This model surpassed all baselines in performance-efficiency trade-offs. For comparison, ResNet-18 achieved a similar mIoU (0.840) but at only 40.4 FPS and with over 11 million parameters. MobileOne-S0 attained higher accuracy (0.859 mIoU) but suffered from lower throughput at 25.4 FPS. EfficientNet-B0 showed lower mIoU (0.820) and moderate inference speed (38.5 FPS). Our evolutionary search algorithm demonstrated stable convergence behavior, with consistent improvements in fitness across generations and a clearly dominant position in the Pareto front of the accuracy-latency space. On the Myriad X, which imposes more severe hardware constraints, including the need for 16-bit quantization and strict memory management, our optimized model achieved 0.802 mIoU at 11.5 FPS, with a model size of less than 1K parameters. In contrast, the baselines suffered from significant performance degradation under quantization. EfficientNet-B0 achieved 0.795 mIoU at 4.73 FPS, ResNet-18 yielded 0.770 mIoU at 5.86 FPS, and MobileOne-S0 dropped to 0.683 mIoU at just 4.32 FPS. These declines can be attributed to architectural mismatches between the baseline designs and the Myriad X’s compilation and runtime execution model. In particular, layers such as large-kernel convolutions, SE blocks, and skip connections negatively affected inference speed and accuracy post-quantization. In contrast, the NAS-discovered models were inherently optimized for the Myriad’s constraints, maintaining robustness and latency efficiency. Qualitative assessments of segmentation outputs confirmed the advantages of the NAS-generated models. In representative scenes, both the Jetson and Myriad variants produced predictions with higher spatial fidelity, more accurate burn region delineation, and improved structural alignment with ground truth labels compared to the baseline models. The Jetson-optimized variant in particular achieved superior contour precision in complex cases involving small or fragmented burn scars. Overall, these results confirm the ability of the proposed framework to generate compact and efficient models tailored to diverse deployment scenarios. By integrating hardware profiling directly into the search process, the framework consistently discovered architectures that outperformed state-of-the-art baselines in both accuracy and speed while operating under tight resource budgets. This demonstrates a significant step forward in enabling real-time, autonomous Earth observation from orbit.